{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple neural network\n",
    "\n",
    "The program creates an neural network that simulates the exclusive OR function with two inputs and one output. \n",
    "\n",
    "O programa cria uma rede neural que simula a função OR exclusiva com duas entradas e uma saída."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Isso importa numpy, biblioteca de álgebra linear. Esta é a nossa única dependência.\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos criar uma função que vai mapear qualquer valor para um valor 0 ou 1. Isso é chamado de Sigmoide. Essa função vai rodar em cada neurônio da nossa rede neural quando a informação chegar. Isso é útil para criar probabilidades a partir de números. Assim que tivermos criado isso, vamos inicializar nosso dataset como uma matriz.\n",
    "\n",
    "A seguir está uma definição da função sigmóide, que é o tipo de não-linearidade escolhida para esta rede neural. Não é o único tipo de não-linearidade que pode ser escolhido, porém é de simples compreeção e tem as características analíticas necessárias.\n",
    "\n",
    "Na prática, os sistemas de deep learning de grande escala utilizam funções lineares por partes, uma vez que são muito menos dispendiosas de avaliar.\n",
    "\n",
    "Se a flag deriv for True, a função calcula a derivada da função, que é usada no passo de backpropogation erro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def nonlin(x, deriv=False):  # Note: there is a typo on this line in the video\n",
    "    if(deriv==True):\n",
    "        return (x*(1-x))\n",
    "    \n",
    "    return 1/(1+np.exp(-x))  # Note: there is a typo on this line in the video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O código a seguir cria a matriz de entrada. A terceira coluna é para acomodar o termo bias e não faz parte da entrada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#input data\n",
    "#Matriz de dados de entrada onde cada linha é um exemplo de treinamento\n",
    "X = np.array([[0,0,1],  \n",
    "            [0,1,1],\n",
    "            [1,0,1],\n",
    "            [1,1,1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Acima criamos nosso dataset sendo que cada linha é um exemplo de treinamento diferente.\n",
    "\n",
    "Isso inicializa nosso dataset de entrada como uma matriz numpy. Cada linha é um único \"exemplo de treinamento\". Cada coluna corresponde a um dos nossos nós de entrada ou seja a um neurônio. Assim, temos 3 nós de entrada para a rede e 4 exemplos de treinamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#output data\n",
    "#A saída da função OR exclusiva.\n",
    "#Matriz de conjunto de dados de saída onde cada linha é um exemplo de treinamento\n",
    "y = np.array([[0],\n",
    "             [1],\n",
    "             [1],\n",
    "             [0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos um seed para o nosso random, queremos torná-lo determinístico. Assim ele vai gerar números aleatórios a partir do mesmo princípio ou seed, assim geramos a mesma sequência toda vez que rodarmos o programa. Isso pode ser útil em caso de depuração."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora vamos criar nossas matrizes de sinapses. Sinapses são as conexões entre cada neurônio de cada camada para cada neurônio da camada siguinte. \n",
    "\n",
    "Já que temos 3 camadas na nossa rede, vamos precisar de 2 matrizes de sinapses. Cada sinapse tem um peso aleatório associado a ela. \n",
    "\n",
    "Now we intialize the weights to random values. syn0 are the weights between the input layer and the hidden layer.  It is a 3x4 matrix because there are two input weights plus a bias term (=3) and four nodes in the hidden layer (=4). syn1 are the weights between the hidden layer and the output layer. It is a 4x1 matrix because there are 4 nodes in the hidden layer and one output. Note that there is no bias term feeding the output layer in this example. The weights are initially generated randomly because optimization tends not to work well when all the weights start at the same value. Note that neither of the neural networks shown in the video describe the example. \n",
    "\n",
    "Agora nós inicializamos os pesos para os valores aleatórios. **syn0** são os pesos entre a camada de entrada e a camada oculta.\n",
    "\n",
    "É uma matriz 3x4 porque há dois pesos de entrada mais um termo de polarização (= 3) e quatro nós na camada oculta (= 4). Syn1 são os pesos entre a camada oculta ea camada de saída. É uma matriz 4x1 porque existem 4 nós na camada oculta e uma saída. Observe que não há nenhum termo de polarização que alimente a camada de saída neste exemplo. Os pesos são inicialmente gerados aleatoriamente porque a otimização tende a não funcionar bem quando todos os pesos começam no mesmo valor. Observe que nenhuma das redes neurais mostradas no vídeo descreve o exemplo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#synapses\n",
    "syn0 = 2*np.random.random((3,4)) - 1  # 3x4 matrix of weights ((2 inputs + 1 bias) x 4 nodes in the hidden layer)\n",
    "syn1 = 2*np.random.random((4,1)) - 1  # 4x1 matrix of weights. (4 nodes x 1 output) - no bias term in the hidden layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora vamos começar o código de treinamento.\n",
    "\n",
    "Primeiro vamos criar nosso loop que vai iterar o código de treinamento e otimizar a rede para um determinado dataset.\n",
    "\n",
    "Vamos criar a primeira camada, que são nossos dados de entrada.\n",
    "\n",
    "Agora vamos a previsão, vamos fazer uma multiplicação de matrizes entre cada camada e a sinapse. Depois vamos rodar nossa função Sigmoide em todos os valores da matriz para criar a próxima camada. A próxima camada contém a previsão de dados de saída. Então vamos fazer a mesma coisa nessa camada para obter a próxima camada, que é uma previsão mais refinada.\n",
    "\n",
    "Então agora que temos uma previsão do valor de saída na camada 2, vamos compará-la com os dados de saída esperados usando subtração para obter nossa taxa de erro. Também vamos plotar a taxa de erro média em um dado intervalo para garantir que ele esteja sempre diminuindo. Agora vamos multiplicar nossa taxa de erro pelo resultado da nossa função Sigmoide.\n",
    "\n",
    "A função é usada para obter a derivada das nossas previsões de saída da camada 2. Isso vai nos dar um delta que vamos usar para reduzir a taxa de erro das previsões quando atualizarmos nossas sinapses em cada iteração.\n",
    "\n",
    "Então veremos o quanto a camada 1 contribuiu para o erro na camada 2. Isso é chamado de **retropropagação** ou **BackPropagation**.\n",
    "\n",
    "Vamos obter esse erro multiplicado o delta da camada 2 pela transposta da Sinapse 1. Então vamos obter o delta da camada 1 multiplicando seu erro pelo resultado da nossa função Sigmoide. A função é usada para obter a derivada da camada 1. Agora que temos os deltas de cada camada, podemos usá-los para atualizar os pesos das sinaspes para reduzir a taxa de erro cada vez mais a cada iteração. Esse algoritmo é chamado de **gradiente descendente** ou **Gradient Descent**.\n",
    "\n",
    "Para fazer isso vamos simplesmente multiplicar cada erro por um delta. Agora vamos vamos plotar a saída prevista.\n",
    "\n",
    "A saída mostra a evolução do erro entre o modelo e o desejado. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 0.00318350238587\n",
      "Error: 0.00293230634228\n",
      "Error: 0.00273150641821\n",
      "Error: 0.00256631724004\n",
      "Error: 0.00242737608676\n",
      "Error: 0.00230843116063\n",
      "Error: 0.00220512937522\n",
      "Output after training\n",
      "[[ 0.00173654]\n",
      " [ 0.99784146]\n",
      " [ 0.99799983]\n",
      " [ 0.00256212]]\n"
     ]
    }
   ],
   "source": [
    "#training step\n",
    "# Python2 Note: In the follow command, you may improve \n",
    "#   performance by replacing 'range' with 'xrange'. \n",
    "for j in range(60000):  \n",
    "    \n",
    "    # Calculate forward through the network.\n",
    "    l0 = X #Primeira camada da rede, especificada pelos dados de entrada\n",
    "    l1 = nonlin(np.dot(l0, syn0)) #Segunda camada da rede, também conhecida como a camada oculta\n",
    "    l2 = nonlin(np.dot(l1, syn1))\n",
    "    \n",
    "    # Back propagation of errors using the chain rule. \n",
    "    l2_error = y - l2\n",
    "    if(j % 10000) == 0:   # Only print the error every 10000 steps, to save time and limit the amount of output. \n",
    "        print(\"Error: \" + str(np.mean(np.abs(l2_error))))\n",
    "        \n",
    "    l2_delta = l2_error*nonlin(l2, deriv=True)\n",
    "    \n",
    "    l1_error = l2_delta.dot(syn1.T)\n",
    "    \n",
    "    l1_delta = l1_error * nonlin(l1,deriv=True)\n",
    "    \n",
    "    #update weights (no learning rate term)\n",
    "    syn1 += l1.T.dot(l2_delta)\n",
    "    syn0 += l0.T.dot(l1_delta)\n",
    "    \n",
    "print(\"Output after training\")\n",
    "print(l2)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver como nossa taxa de erro diminui a cada iteração.\n",
    "\n",
    "Veja como a saída final aproxima-se da saída verdadeira [0, 1, 1, 0]. Se você aumentar o número de interações no loop de treinamento (atualmente 60000), a saída final será ainda mais próxima.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
