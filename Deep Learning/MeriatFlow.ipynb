{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# O que é uma Rede Neural Artificial?\n",
    "\n",
    "Uma rede neural artificial é um grafo composto por funções matemáticas como combinações lineares e funções de ativação. O grafo consiste de nós e arestas.\n",
    "\n",
    "Nós em cada camada (exceto por nós na camada de entrada), realizam funções matemáticas usando como entrada a saída das camadas anteriores. Por exemplo, um nó pode representar `f(x,y)=x+y`, onde `x` e `y` são valores de entrada vindos da camada anterior.\n",
    "\n",
    "De forma similar, cada nó cria uma saída que pode ser passada para os nós da próxima camada. O valor de saída da camada de saída não é passada para uma camada futura (pois é a última!).\n",
    "\n",
    "Camadas entre a camada de entrada e a de saída são chamadas de camadas ocultas.\n",
    "\n",
    "## Propagação para a Frente\n",
    "\n",
    "Ao propagar valores da primeira cada (camada de entrada) através das funções matemáticas apresentadas em cada nó, a rede emite um valor de saída. Este processo é chamado de propagação para a frente, ou forward pass.\n",
    "\n",
    "Aqui está um exemplo simples de propagação para a frente.\n",
    "\n",
    "## Grafos\n",
    "\n",
    "Os nós e arestas criam uma estrutura de grafo. Apesar de que o exemplo acima é razoavelmente simples, não é difícil imaginar que grafos crescentemente complexos sejam capazes de calcular... bem... qualquer qualquer coisa.\n",
    "\n",
    "Existem geralmente dois passos para a criação de uma rede neural:\n",
    "\n",
    "1. Definir o grafo de nós e arestas.\n",
    "2. Propagar os valores dentro do grafo.\n",
    "\n",
    "O `NanoFlow` trabalha da mesma forma. Você definirá os nós e arestas da sua rede com um método e irá propagar os valores dentro do grafo usando outro método. \n",
    "\n",
    "Sabemos que cada nó pode receber entradas de diversos outros nós. Sabemos também que cada nó cria uma saída única, que muito provavelmente será passada para outros nós. Vamos adicionar duas listas então: uma para adicionar referências aos nós de entrada, e outra para as referências dos nós de saída.\n",
    "\n",
    "Sabemos que cada nó pode receber entradas de diversos outros nós. Sabemos também que cada nó cria uma saída única, que muito provavelmente será passada para outros nós. Vamos adicionar duas listas então: uma para adicionar referências aos nós de entrada, e outra para as referências dos nós de saída."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Node(object):\n",
    "    def __init__(self, inbound_nodes=[]):\n",
    "        # Nó(s) que fornecem valores para o nó corrente. Referência aos nós de entrada.\n",
    "        self.inbound_nodes = inbound_nodes\n",
    "        # Nó(s) aos quais o nó corrente passa valores. Referência aos nós de saída.\n",
    "        self.outbound_nodes = []\n",
    "        # Para cada nó  aqui, adicionamos este nó como um nó de saída *daquele* nó.\n",
    "        for n in self.inbound_nodes:\n",
    "            n.outbound_nodes.append(self)\n",
    "        # Cada nó eventualmente calculará um valor que representa a sua saída\n",
    "        self.value = None\n",
    "        \n",
    "    # Cada nó será capaz de passar valores \"forward propagation\" e \"back propagation\".\n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        Propagação para a frente.\n",
    "\n",
    "        Calcula o valor de saída baseando-se nos `inbound_nodes` e\n",
    "        armazena o valor final em self.value.\n",
    "        \"\"\"\n",
    "        raise NotImplemented"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A subclasse Node\n",
    "Diferentemente das outras subclasses de `Node`, a subclasse `Input` não calcula nada. A subclasse `Input` apenas armazena um valor `value`, como um dado de um atributo ou um parâmetro do modelo (peso/viés).\n",
    "\n",
    "Você pode definir o valor `value` de forma explícita ou usando o método `forward()`. Este valor é usado para alimentar o restante da rede neural."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Input(Node):\n",
    "    # Pense em Input como o agrupamento de múltiplos nós de entrada individuais em um único Nó.\n",
    "    def __init__(self):\n",
    "        # Um nó Input não possui nós de entrada,\n",
    "        # então não há necessidade de passar nada para o construtor.\n",
    "        Node.__init__(self)\n",
    "\n",
    "    # NOTA: O nó Input é o único onde o valor pode\n",
    "    # ser passado como um argumento para forward().\n",
    "    #\n",
    "    # Todas as outras implementações de nós devem receber o valor\n",
    "    # do nó anterior disponível em self.inbound_nodes\n",
    "    #\n",
    "    # Exemplo:\n",
    "    # val0 = self.inbound_nodes[0].value\n",
    "    def forward(self, value=None):\n",
    "        # Sobre-escreva o valor se um valor foi fornecido.\n",
    "        if value is not None:\n",
    "            self.value = value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A subclasse Add\n",
    "\n",
    "Note a diferença no método `__init__: Add.__init__(self, [x, y])`. Diferentemente da classe `Input`, que não possui nós como entrada, a subclasse `Add` recebe dois nós no momento de sua criação, x e y, e adiciona os valores destes nós.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Can you augment the Add class so that it accepts any number of nodes as input?\n",
    "\n",
    "Hint: this may be useful:\n",
    "https://docs.python.org/3/tutorial/controlflow.html#unpacking-argument-lists\n",
    "\"\"\"\n",
    "class Add(Node):\n",
    "    # Utilizo o construtor para passar os \"inbound_nodes\" isnanciados na classe Node\n",
    "    def __init__(self, x, y):\n",
    "        Node.__init__(self, [x, y])\n",
    " \n",
    "    def forward(self):\n",
    "        x_value = self.inbound_nodes[0].value\n",
    "        y_value = self.inbound_nodes[1].value\n",
    "        self.value = x_value + y_value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NanoFlow possui dois métodos para te ajudar a definir e então executar valores dentro de seus grafos: **topological_sort()** e **forward_pass()**.\n",
    "\n",
    "### topological_sort\n",
    "\n",
    "Para definir a sua rede, você precisará definir a ordem das operações para os seus nós. Dado que a entrada de alguns nós depende da saída de outros nós, você precisará encadear o grafo de forma que as dependências das entradas de cada um dos nós estejam disponíveis antes de executar o seu cálculo. Esta técnica é chamada de [ordenação topológica](https://en.wikipedia.org/wiki/Topological_sorting).\n",
    "\n",
    "A função `topological_sort()` implementa a ordenação topológica usando o [Algoritmo de Kahn](https://en.wikipedia.org/wiki/Topological_sorting#Kahn.27s_algorithm). Os detalhes deste método não são importantes neste momento, mas seu resultado é, `topological_sort()` retorna uma lista de nós em uma sequência que permite que todos os cálculos sejam realizados de forma serializada (em série). O método `topological_sort()` recebe como entrada um `feed_dict`, que é como nós inicializamos um valor para um nó `Input`. O `feed_dict` é representado na forma de um dicionário Python. **Aqui está um exemplo de uso:**\n",
    "\n",
    "```python\n",
    "# Define 2 nós do tipo \"Input\".\n",
    "x, y = Input(), Input()\n",
    "# Define um nó do tipo \"Add\" e usa os dois nós do tipo \"Input\" como entrada.\n",
    "add = Add(x, y)\n",
    "# Os valores de \"x\" e \"y\" serão definidos como 10 e 20, respectivamente.\n",
    "feed_dict = {x: 10, y: 20}\n",
    "# Ordena os nós usando ordenação topológica.\n",
    "sorted_nodes = topological_sort(feed_dict=feed_dict)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def topological_sort(feed_dict):\n",
    "    \"\"\"\n",
    "    Sort the nodes in topological order using Kahn's Algorithm.\n",
    "\n",
    "    `feed_dict`: A dictionary where the key is a `Input` Node and the value is the respective value feed to that Node.\n",
    "\n",
    "    Returns a list of sorted nodes.\n",
    "    \"\"\"\n",
    "\n",
    "    input_nodes = [n for n in feed_dict.keys()]\n",
    "\n",
    "    G = {}\n",
    "    nodes = [n for n in input_nodes]\n",
    "    while len(nodes) > 0:\n",
    "        n = nodes.pop(0)\n",
    "        if n not in G:\n",
    "            G[n] = {'in': set(), 'out': set()}\n",
    "        for m in n.outbound_nodes:\n",
    "            if m not in G:\n",
    "                G[m] = {'in': set(), 'out': set()}\n",
    "            G[n]['out'].add(m)\n",
    "            G[m]['in'].add(n)\n",
    "            nodes.append(m)\n",
    "\n",
    "    L = []\n",
    "    S = set(input_nodes)\n",
    "    while len(S) > 0:\n",
    "        n = S.pop()\n",
    "\n",
    "        if isinstance(n, Input):\n",
    "            n.value = feed_dict[n]\n",
    "\n",
    "        L.append(n)\n",
    "        for m in n.outbound_nodes:\n",
    "            G[n]['out'].remove(m)\n",
    "            G[m]['in'].remove(n)\n",
    "            # if no other incoming edges add to S\n",
    "            if len(G[m]['in']) == 0:\n",
    "                S.add(m)\n",
    "    return L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### forward_pass\n",
    "\n",
    "O outro método disponível é o **forward_pass()**, que de fato executa a rede e emite uma saída."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forward_pass(output_node, sorted_nodes):   \n",
    "    \"\"\"\n",
    "    Performs a forward pass through a list of sorted nodes.\n",
    "\n",
    "    Arguments:\n",
    "        `output_node`: A node in the graph, should be the output node (have no outgoing edges). O nó de saída do grafo (sem arestas de saída).\n",
    "        `sorted_nodes`: A topologically sorted list of nodes. Uma lista topologicamente ordenada de nós.\n",
    "\n",
    "    Returns the output Node's value\n",
    "    \"\"\"\n",
    "\n",
    "    for n in sorted_nodes:\n",
    "        n.forward()\n",
    "\n",
    "    return output_node.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forward_pass_v2(graph):\n",
    "    \"\"\"\n",
    "    Executa o forward pass através de uma lista de Nodes ordenados.\n",
    "\n",
    "    Arguments:\n",
    "        `graph`: The result of calling `topological_sort`.\n",
    "    \"\"\"\n",
    "    # Forward pass\n",
    "    for n in graph:\n",
    "        n.forward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A subclasse Linear\n",
    "\n",
    "As redes neurais recebem entradas e produzem saídas. Redes neurais podem melhorar a acurácia de sua saída com o tempo.\n",
    "\n",
    "Um neurônio artificial simples depende de três componentes:\n",
    "\n",
    "* entradas, xi\n",
    "* pesos, wi\n",
    "* viés, b\n",
    "\n",
    "A saída, y, é a soma ponderada das entradas mais o viés.\n",
    "\n",
    "Ao variar os pesos, você pode variar a influência que qualquer entrada tenha na saída. O aspecto de aprendizagem de redes neurais se dá durante um processo chamado retro-propagação (backpropagation). Na retro-propagação, a rede modifica os pesos para melhorar a acurácia da saída da rede. Você irá aplicar estes conceitos em breve.\n",
    "\n",
    "A função Linear é um neurônio linear que gera uma saída ao aplicar uma versão simplificada de uma soma ponderada. A função Linear deve receber como entrada **n** nós de entrada, uma lista de pesos também com tamanho **n** e um viés (bias).\n",
    "\n",
    "Álgebra linear reflete bem a idéia de transformar valores em camadas em um grafo. Existe uma técnica chamada **transformation** que realiza exatamente o que uma **camada** deve fazer - ela converte entradas em saídas em diversas dimensões. Observe a equação de saída:\n",
    "\n",
    "![1](https://d17h27t6h515a5.cloudfront.net/topher/2017/February/5892a66c_neuron-output/neuron-output.png)\n",
    "\n",
    "Durante os cáculos, vamos denotar **x** como **X** e **w** como **W** pois estes são agora matrizes. Neste cenário **b** será um vetor ao invés de um valor escalar.\n",
    "\n",
    "Considere o nó **Linear** com uma entrada e **k** saídas (mapeando 1 entrada para k saídas). Neste contexto, uma entrada ou saída são sinônimos de atributos. Neste caso, **X** é uma matriz de 1 por 1.\n",
    "\n",
    "![1](https://d17h27t6h515a5.cloudfront.net/topher/2016/November/581f9571_newx/newx.png)\n",
    "\n",
    "**W** se torna uma matriz de ordem 1 por **k** (se parece como uma linha).\n",
    "\n",
    "![1](https://d17h27t6h515a5.cloudfront.net/topher/2016/November/581f9571_neww/neww.png)\n",
    "\n",
    "O resultado da multiplicação das matrizes **X** e **W** é uma matriz de ordem 1 por **k**. Como **b** também é uma matriz de 1 por **k**, **b** é adicionado diretamente a saída da multiplicação de **X** e **W**. E se quiséssemos mapear **n** entradas para **k** saídas?\n",
    "\n",
    "Então **X** seria uma matriz de 1 por **n** e **W** uma matriz de **n** por **k**. O resultado desta multiplicação ainda seria uma matriz de 1 por **k**, então o uso dos viéses continuam os mesmos.\n",
    "\n",
    "![1](https://d17h27t6h515a5.cloudfront.net/topher/2016/November/581f9570_newx-1n/newx-1n.png)\n",
    "![1](https://d17h27t6h515a5.cloudfront.net/topher/2017/February/58a24e51_neww-nk-fixed/neww-nk-fixed.gif)\n",
    "![1](https://d17h27t6h515a5.cloudfront.net/topher/2016/November/581a94e5_b-1byk/b-1byk.png)\n",
    "\n",
    "Vamos ver agora um exemplo com **n** entradas. Considere uma imagem em escala de cinza de tamanho **28px** por **28px** (como o dataset MNIST). Nós podemos reformatar esta imagem para que ela se torne uma matriz de ordem **1 por 784**, ou seja, **n = 784**. Cada pixel é uma entrada/atributo.\n",
    "\n",
    "Na prática, o padrão é usar várias entradas simultaneas durante a \"passagem para a frente\" de uma rede neural. Não é factível usar uma entrada por vez. O principal motivo para isto é que cada um dos exemplos pode ser processado em paralelo, resultando em grandes melhorias em performance. O número de entradas é chamado de **batch size**. Números comuns para este parâmetro são 32, 64, 128, 256 e 512. Normalmente, estes são os máximos que conseguimos comportar em memória. O que isso significa para **X**, **W** e **b**?\n",
    "\n",
    "**X** se torna uma matriz de **m** por **n** enquanto **W** e **b** se mantém iguais. O resultado da multiplicação de matrizes é agora **m** por **k**, então a adioção de **b** é o resultado de um broadcast sobre cada linha."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Linear(Node):\n",
    "    #def __init__(self, inputs, weights, bias):\n",
    "    def __init__(self, X, W, b):\n",
    "        #Node.__init__(self, [inputs, weights, bias])\n",
    "        Node.__init__(self, [X, W, b])\n",
    "\n",
    "        # NOTE: The weights and bias properties here are not\n",
    "        # numbers, but rather references to other nodes.\n",
    "        # The weight and bias values are stored within the\n",
    "        # respective nodes.\n",
    "\n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        Set self.value to the value of the linear function output. y=∑wi xi + b\n",
    "        \n",
    "        Nesta solução, eu defini self.value como sendo o viés (bias) e então iterei sobre os valores de entrada \n",
    "        e pesos, adicionando cada entrada já ponderada em self.value. Note que chamar .value em self.inbound_nodes[0] \n",
    "        ou self.inbound_nodes[1] nos retorna uma lista.\n",
    "        \"\"\"\n",
    "        #inputs = self.inbound_nodes[0].value\n",
    "        #weights = self.inbound_nodes[1].value\n",
    "        #bias = self.inbound_nodes[2]\n",
    "        #self.value = bias.value\n",
    "        #for x, w in zip(inputs, weights):\n",
    "        #    self.value += x * w\n",
    "            \n",
    "        \"\"\"\n",
    "        Na solução v2, estou preparando o NanoFlow para trabalhar com matrizes e vetores.\n",
    "        Você precisará usar o método np.dot, que trabalha como uma multiplicação de matrizes 2D, para multiplicar as matrizes de entrada e pesos da Equação (2).\n",
    "        É importante também notar que o numpy, na prática, sobrecarrega o operador __add__ para que você possa usá-lo diretamente com estruturas do tipo np.array (eg. np.array() + np.array()).\n",
    "        Obtive os valores de X, W e b das suas respectivas entradas. Eu usei o método np.dot para realizar a multiplicação de matrizes.\n",
    "        \"\"\"\n",
    "        X = self.inbound_nodes[0].value\n",
    "        W = self.inbound_nodes[1].value\n",
    "        b = self.inbound_nodes[2].value\n",
    "        self.value = np.dot(X, W) + b\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Sigmoid\n",
    "\n",
    "Redes neurais tiram vantagem de diferentes transformações e funções de ativação para melhorar as suas saídas. **Transformações lineares** são ótimas para aplicar desvios a valores, mas redes neurais normalmente requerem transformações mais complexas. Por exemplo, um dos primeiros projetos de neurônio artificial, o [perceptron](https://en.wikipedia.org/wiki/Perceptron), exibia um comportamento de saída binário. Perceptrons comparam uma entrada ponderada com um limiar. QUando a saída excede este limiar, o perceptron é ativado e a saída se torna 1, ou em caso contrário, 0.\n",
    "\n",
    "A idéia do comportamento binário geralmente faz sentido para problemas de classificação. Por exemplo, se você pedir para uma rede hipotetizar se uma imagem manuscrita é um '9', estamos falando de uma saída binária - sim, é um **'9'**, ou não, este não é um **'9'**. No caso do perceptron podemos usar a **step function** ou **função degrau** que é uma das formas mais simples de saída binária. O problema é que este tipo de função não é contínua e diferenciável, o que é muito ruim, já que a **diferenciação** é o que torna o processo de **descida do gradiente** possível.\n",
    "\n",
    "![sigmoid](https://d17h27t6h515a5.cloudfront.net/topher/2016/October/58102c91_19/19.png)\n",
    "\n",
    "Já a **função sigmoide**, apresentada na equação acima, substitui a limiarização com uma bela curva em formato de **S**, que funciona como uma limiarização, mas é capaz de manter a continuidade e também a diferenciação. Como um bônus, a função sigmoide tem uma derivada que é muito parecida com a própria sigmoide original.\n",
    "\n",
    "Note que a função sigmoide possui apenas um parâmetro. Lembre-se que a sigmoide é uma função de ativação (não linear), o que significa que ela recebe uma entrada única e realiza uma operação matemática sobre este valor.\n",
    "\n",
    "Conceitualmente, a função sigmoide toma decisões. Quando alimentada com atributos ponderados sobre dados, essa função indica se os atributos contribuem ou não para a classificação. Deste modo, uma sigmoide trabalha bem seguida de uma função linear. Até o momento, usando pesos e viéses aleatórios, a saída de um nó sigmoide é também aleatório. O processo de aprendizagem usando retro-propagação e descida do gradiente, modifica os pesos e viéses, e com isso, a saída da ativação começará a aderir às saídas esperadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Sigmoid(Node):\n",
    "    def __init__(self, node):\n",
    "        Node.__init__(self, [node])\n",
    "\n",
    "    def _sigmoid(self, x):\n",
    "        \"\"\"\n",
    "        Este método está isolado da função \"forward\" pois será usada na função \"backward\" também.\n",
    "\n",
    "        \"x\": Um objeto em formato de array numpy.\n",
    "        \"\"\"\n",
    "        return 1. / (1. + np.exp(-x)) # o `.` garante que `1` é um float\n",
    "\n",
    "    def forward(self):\n",
    "        input_value = self.inbound_nodes[0].value\n",
    "        self.value = self._sigmoid(input_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Squared Error\n",
    "\n",
    "Como você deve lembrar, redes neurais melhoram a acurácia de suas saídas ao modificar os pesos e viéses das suas saídas em resposta ao dados rotulados de treinamento.\n",
    "\n",
    "Existem diferentes técnicas para definir a acurácia de uma rede neural, sendo que todas estão focadas na habilidade das redes em aproximar as suas saídas às saídas esperadas. Diferentes métricas são usadas para medir a acurácia, normalmente sendo denominadas como **perda (loss)** ou **custo (cost)**.\n",
    "\n",
    "![mse](https://d17h27t6h515a5.cloudfront.net/topher/2016/December/585ab25f_codecogseqn/codecogseqn.png)\n",
    "\n",
    "Aqui, w denota a coleção de todos os pesos na rede, b todos os viéses, m é o número de exemplos de treinamento, a é uma aproximação de y(x) dada pela rede, sendo que a e y(x) são vetores de mesmo tamanho.\n",
    "\n",
    "A coleção de pesos são todas as matrizes de peso vetorizadas e concatenadas em um grande vetor. O mesmo ocorre para a coleção de viéses exceto pelo fato de que elas já são vetores e não existe necessidade de vetorizá-las, mas apenas concatená-las. Aqui está um exemplo da criação de w em código:\n",
    "\n",
    "```python\n",
    "# matrizes 2 por 2\n",
    "w1  = np.array([[1, 2], [3, 4]])\n",
    "w2  = np.array([[5, 6], [7, 8]])\n",
    "\n",
    "# vetorização\n",
    "w1_flat = np.reshape(w1, -1)\n",
    "w2_flat = np.reshape(w2, -1)\n",
    "\n",
    "w = np.concatenate((w1_flat, w2_flat))\n",
    "# array([1, 2, 3, 4, 5, 6, 7, 8])\n",
    "```\n",
    "\n",
    "A matemática por trás do MSE reflete acima, onde **y** é a saída desejada e **a** é a saída computada pela rede neural. Nós então calculamos o quadrado da diferença `diff**2` (ou alternativamente np.square(diff)). Por fim, nós precisamos somar os quadrados das diferenças e dividir pelo número total de exemplos m. Isto pode ser feito com a função `np.mean` ou calculando `(1 /m) * np.sum(diff**2)`.\n",
    "\n",
    "Esta é uma maneira interessante de abstrair todos os pesos e viéses usados na rede neural e torna as coisas mais simples de se escrever como veremos em breve nas seções sobre descida do gradiente.\n",
    "\n",
    "O custo, `C`, depende da diferença entre a saída correta, `y(x)`, e a saída fornecida pela rede, `a`. É fácil verificar que não há diferença entre `y(x)` e `a` (para todos os valores de x), o que nos leva a um custo `0`.\n",
    "\n",
    "Esta é uma situação ideal, e o processo de aprendizagem tenta minimizar o custo quanto for possível."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MSE(Node):\n",
    "    def __init__(self, y, a):\n",
    "        \"\"\"\n",
    "        The mean squared error cost function.\n",
    "        Should be used as the last node for a network.\n",
    "        \"\"\"\n",
    "        # Call the base class' constructor.\n",
    "        Node.__init__(self, [y, a])\n",
    "\n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        Calculates the mean squared error.\n",
    "        \"\"\"\n",
    "        # NOTE: We reshape these to avoid possible matrix/vector broadcast\n",
    "        # errors.\n",
    "        #\n",
    "        # For example, if we subtract an array of shape (3,) from an array of shape\n",
    "        # (3,1) we get an array of shape(3,3) as the result when we want\n",
    "        # an array of shape (3,1) instead.\n",
    "        #\n",
    "        # Making both arrays (3,1) insures the result is (3,1) and does\n",
    "        # an elementwise subtraction as expected.\n",
    "        y = self.inbound_nodes[0].value.reshape(-1, 1)\n",
    "        a = self.inbound_nodes[1].value.reshape(-1, 1)\n",
    "        m = self.inbound_nodes[0].value.shape[0]\n",
    "\n",
    "        diff = y - a\n",
    "        self.value = np.mean(diff**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Linear Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-9.  4.]\n",
      " [-9.  4.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "X, W, b = Input(), Input(), Input()\n",
    "\n",
    "f = Linear(X, W, b)\n",
    "\n",
    "X_ = np.array([[-1., -2.], [-1, -2]])\n",
    "W_ = np.array([[2., -3], [2., -3]])\n",
    "b_ = np.array([-3., -5])\n",
    "\n",
    "feed_dict = {X: X_, W: W_, b: b_}\n",
    "\n",
    "graph = topological_sort(feed_dict)\n",
    "output = forward_pass(f, graph)\n",
    "\n",
    "\"\"\"\n",
    "Output should be:\n",
    "[[-9., 4.],\n",
    "[-9., 4.]]\n",
    "\"\"\"\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  1.23394576e-04   9.82013790e-01]\n",
      " [  1.23394576e-04   9.82013790e-01]]\n"
     ]
    }
   ],
   "source": [
    "X, W, b = Input(), Input(), Input()\n",
    "\n",
    "f = Linear(X, W, b)\n",
    "g = Sigmoid(f)\n",
    "\n",
    "X_ = np.array([[-1., -2.], [-1, -2]])\n",
    "W_ = np.array([[2., -3], [2., -3]])\n",
    "b_ = np.array([-3., -5])\n",
    "\n",
    "feed_dict = {X: X_, W: W_, b: b_}\n",
    "\n",
    "graph = topological_sort(feed_dict)\n",
    "output = forward_pass(g, graph)\n",
    "\n",
    "\"\"\"\n",
    "Output should be:\n",
    "[[  1.23394576e-04   9.82013790e-01]\n",
    " [  1.23394576e-04   9.82013790e-01]]\n",
    "\"\"\"\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23.4166666667\n"
     ]
    }
   ],
   "source": [
    "y, a = Input(), Input()\n",
    "cost = MSE(y, a)\n",
    "\n",
    "y_ = np.array([1, 2, 3])\n",
    "a_ = np.array([4.5, 5, 10])\n",
    "\n",
    "feed_dict = {y: y_, a: a_}\n",
    "graph = topological_sort(feed_dict)\n",
    "# forward pass\n",
    "forward_pass_v2(graph)\n",
    "\n",
    "\"\"\"\n",
    "Expected output\n",
    "\n",
    "23.4166666667\n",
    "\"\"\"\n",
    "print(cost.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
