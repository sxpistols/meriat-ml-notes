{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Probability Model\n",
    "\n",
    "The following example assumes all points in the unit square are equally likely to occur. Let *A* be the event that a randomly selected point is in some finite region contained in the sample space. As probabalistic model we let the probability of the event *A* be equal to the area of *A*. Selecting *N* random *(x,y)* pairs using a uniform distribution, the observed values of the events $A_1, A_2, A_3\\hspace{1pt}$ are compared to the theoretical model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pyplot' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-b4aba0c5a74e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mpyplot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'k'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m#calculate observed probabilities\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pyplot' is not defined"
     ]
    }
   ],
   "source": [
    "from numpy import random as r\n",
    "\n",
    "#draw a uniform random sample on the unit square\n",
    "N = 500\n",
    "sample = r.rand(2,N)\n",
    "pyplot.scatter(sample[0],sample[1], c='k', alpha = 0.3)\n",
    "\n",
    "#calculate observed probabilities\n",
    "def inBoundsCount(v,x1,x2,y1,y2):\n",
    "    if v[0]>=x1 and v[0]<=x2 and v[1]>=y1 and v[1]<=y2: return 1\n",
    "    else: return 0\n",
    "\n",
    "def pA(x1, x2, y1, y2):\n",
    "    return reduce(lambda s,v: s + inBoundsCount(v, x1,x2,y1,y2), sample.T, 0)/float(N)\n",
    "\n",
    "print(\"\\t\\tTheoretical \\t Observed\")\n",
    "print(\"p(A1)\\t\\t{0}\\t\\t{1}\".format(.5*.7, pA(0, .7, .5, 1.0)))\n",
    "print(\"p(A1 & A2)\\t{0}\\t\\t{1}\".format(.2*.3,pA(.5, .7, .5, .8)))\n",
    "print(\"p(A1 & A2 & A3)\\t{0}\\t\\t{1}\".format(.2*.1, pA(.5, .7, .5, .6)))\n",
    "\n",
    "\n",
    "#fill in regions corresponding to events A1, A2, A3\n",
    "#args to fill are X coords, Y coords\n",
    "pyplot.fill([0, .7, .7, 0], [.5, .5, 1, 1],'b',alpha=0.1)\n",
    "pyplot.fill([.5, 1.0, 1.0, .5], [.4, .4, .8, .8], 'r', alpha = 0.1)\n",
    "pyplot.fill([.3, .9, .9, .3], [0, 0, .6, .6], 'y', alpha = 0.2)\n",
    "pyplot.fill([.5, .7, .7, .5], [.5, .5, .6,.6], 'k', alpha = .3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Limit Theorems\n",
    "\n",
    "\n",
    "Consider a sequence $X_1, X_2, \\ldots \\hspace{1pt}$ of independent i.i.d. random variables with mean $\\mu\\hspace{1pt}$ and variance $\\sigma^2\\hspace{1pt}$. Let \n",
    "\n",
    "$$S_n = \\sum_{i=1}^n X_i$$\n",
    "\n",
    "be a partial sum of the random variables. By **independence** we have\n",
    "\n",
    "$$var\\left(S_n\\right) = \\sum_{i=1}^n var\\left(X_i\\right) = n \\sigma^2$$\n",
    "\n",
    "We now define a new random variable, called the **sample mean** given by \n",
    "\n",
    "$$M_n = \\frac{1}{n}\\sum_{i=1}^n X_i = \\frac{S_n}{n}$$\n",
    "\n",
    "which has expected value and variance\n",
    "\n",
    "$$E\\left[M_n\\right] = \\mu \\quad var\\left(M_n\\right) = \\frac{\\sigma^2}{n}$$\n",
    "\n",
    "Notice that the variance of the sample mean decreases to zero as *n* increases, implying that most of the probability distribution for $M_\\hspace{1pt}$ is close to the mean value. \n",
    "\n",
    "We also introduce the random variable \n",
    "\n",
    "$$Z_n = \\frac{S_n - n\\mu}{\\sigma \\sqrt{n}}$$\n",
    "\n",
    "for which \n",
    "\n",
    "$$E\\left[Z_n\\right] = 0 \\quad var\\left(Z_n\\right) = 1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markov Inequality\n",
    "\n",
    "\n",
    "If a RV *X* can only take nonnegative values, then\n",
    "\n",
    "$$P\\left(X \\ge a \\right) \\le \\frac{E\\left[X\\right]}{a} \\quad \\forall a \\gt 0$$\n",
    "\n",
    "\n",
    "## Chebyshev Inequality\n",
    "\n",
    "\n",
    "If *X* is a RV with mean $\\mu \\hspace{1pt}$ and variance $\\sigma^2\\hspace{1pt}$, then\n",
    "\n",
    "$$P\\left(\\left| X - \\mu \\right| \\ge c \\right) \\le \\frac{\\sigma^2}{c^2} \\quad \\forall c \\gt 0$$\n",
    "\n",
    "An alternative form of the Chebyshev inequality is obtained by letting $c=k\\sigma\\hspace{1pt}$ where *k* is postive. This gives\n",
    "\n",
    "$$P\\left(\\left| X - \\mu \\right| \\ge k\\sigma \\right) \\le \\frac{1}{k^2} $$\n",
    "\n",
    "which indicates that the probability of an observation of the random variable *X* being more than *k* standard deviations from the mean is less than or equal to $$1/k^2\\hspace{1pt}$. \n",
    "\n",
    "\n",
    "## Weak Law of Large Numbers\n",
    "\n",
    "Let $X_1, X_2, \\ldots \\hspace{1pt}$ be i.i.d. RVs with mean $\\mu\\hspace{1pt}$. For **every** $\\epsilon > 0 \\hspace{1pt}$ \n",
    "\n",
    "$$\\lim_{n\\rightarrow \\infty} P\\left(\\left|M_n - \\mu \\right| \\ge \\epsilon \\right)= 0$$\n",
    "\n",
    "\n",
    "## Convergence in Probability\n",
    "\n",
    "\n",
    "Let $Y_1, Y_2, \\ldots \\hspace{1pt}$ be a sequence of RVs, *not necessarily independent*, and let *a* be a real number. We say that the sequence $Y_n \\hspace{1pt}$ **converges to** *a* **in probability** if for every $\\epsilon \\gt 0 \\hspace{1pt}$ we have\n",
    "\n",
    "$$\\lim_{n\\rightarrow 0} P\\left( \\left| Y_n -a \\right|  \\gt \\epsilon \\right) = 0$$\n",
    "\n",
    "This implies that the probability distribution of the random variables, $Y_n \\hspace{1pt}$ converges to a distribution that is contained within a space of width $2\\epsilon\\hspace{1pt}$ around the point *a*. However this says nothing about the shape of the distribution. \n",
    "\n",
    "This can be rephrased in the following way: For every $\\epsilon \\gt 0 \\hspace{1pt}$ and for any $\\delta \\gt 0 \\hspace{1pt}$, there exists $n_0 \\hspace{1pt}$ such that\n",
    "\n",
    "$$ P\\left( \\left| Y_n -a \\right|  \\gt \\epsilon \\right) \\le \\delta \\quad \\forall n \\ge n_0$$\n",
    "\n",
    "where $\\epsilon \\hspace{1pt}$ is known as the **accuracy** and $\\delta \\hspace{1pt}$ is known as the **confidence**. \n",
    "\n",
    "\n",
    "## The Central Limit Theorem\n",
    "\n",
    "\n",
    "Let $X_1, X_2, \\ldots\\hspace{1pt}$ be a sequence of i.i.d. random variables with common mean $\\mu\\hspace{1pt}$ and variance $\\sigma^2\\hspace{1pt}$ snd define\n",
    "\n",
    "$$Z_n = \\frac{1}{\\sigma \\sqrt{n}} \\left[\\sum_{i=1}^n X_i - n\\mu\\right]$$\n",
    "\n",
    "Then, the **CDF** of $Z_n\\hspace{1pt}$ converges to the standard normal CDF\n",
    "\n",
    "$$\\Phi\\left(z\\right) = \\frac{1}{\\sqrt{2\\pi}} \\int_{-\\infty}^z e^{-x^2/2}dx$$\n",
    "\n",
    "in the sense that \n",
    "\n",
    "$$\\lim_{n\\rightarrow \\infty} P\\left(Z_n \\le z \\right) = \\Phi\\left(z\\right)$$\n",
    "\n",
    "Note that there is an implicit assumption that the **mean and variance**, $\\mu\\hspace{1pt}$ and $\\sigma^2\\hspace{1pt}$, **are finite**. This does not hold for certain power law distributed RVs.\n",
    "\n",
    "### Approximation of Probability of Sum of RVs\n",
    "\n",
    "Let $S_n = X_1 + \\ldots + X_n\\hspace{1pt}$, where the $X_i\\hspace{1pt}$ are i.i.d. RVs each with mean $\\mu\\hspace{1pt}$ and variance $\\sigma^2\\hspace{1pt}$. If *n* is large, the probability $P\\left(S_n \\le c \\right)\\hspace{1pt}$ can be approximated by \n",
    "\n",
    "1. Calculate $z = \\left(c - n \\mu\\right)/\\sigma\\sqrt{n}\\hspace{1pt}$\n",
    "\n",
    "2. Use the approximation\n",
    "\n",
    "$$P\\left(S_n \\le c \\right) \\approx \\Phi\\left(z\\right)$$\n",
    "\n",
    "where $\\Phi\\left(z\\right)\\hspace{1pt}$ is available from standard normal CDF tables.\n",
    "\n",
    "\n",
    "### Approximation to the Binomial\n",
    "\n",
    "If $S_n\\hspace{1pt}$ is a binomial RV with parameters *n* and *p*, with large *n*, and *k, m* are nonnegative integers, then\n",
    "\n",
    "$$P\\left(k \\le S_n \\le m \\right) \\approx \\Phi\\left( \\frac{m + \\frac{1}{2} -np}{\\sqrt{np\\left(1-p\\right)}}\\right) - \n",
    "\\Phi\\left(\\frac{k - \\frac{1}{2} -np}{\\sqrt{np\\left(1-p\\right)}}\\right)$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
